{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7067bc27",
   "metadata": {},
   "source": [
    "## Code playground for the lecture block *Random numbers*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bcd42",
   "metadata": {},
   "source": [
    "In the following, we will explore some of the concepts discussed in the lecture using simulations. Some of the code will have missing parts that you have to fill in. You can also change parts of the code yourself to play around with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be67cd3",
   "metadata": {},
   "source": [
    "### 1. Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66baf040",
   "metadata": {},
   "source": [
    "As a warm-up, we will have a closer look at Bayes Theorem. Below is a function 'simulate_test_development' that simulates testing of a group of subjects for a sickness.\n",
    "\n",
    "The function returns the probability that if the test is positive, the subject is actually sick.\n",
    "\n",
    "**Your task is to complete the function 'bayes' given sensitivity, specificity, and the general incidence (p(sick)), and compare the result with the simulation for various parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7566fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import numpy as np\n",
    "\n",
    "# First, we set the random seed. This way, we will always produce the same random numbers when we run the code.\n",
    "# This is useful, as our experiment becomes reproducible this way!\n",
    "np.random.seed(424242)\n",
    "\n",
    "def simulate_test_development(sensitivity, specificity, incidence_of_infection, number_of_test_subjects):\n",
    "    # We have 'number_of_test_subjects' people in our test population.\n",
    "    # According to the incidence of infection, we randomly assign whether a person is infected or not.\n",
    "    ground_truth = np.random.binomial(1, incidence_of_infection, number_of_test_subjects)\n",
    "    # We sort the array for convenience\n",
    "    ground_truth = np.sort(ground_truth)\n",
    "\n",
    "    # We test the sick subjects\n",
    "    test_result_true_positive = np.random.binomial(1, sensitivity, np.sum(ground_truth))\n",
    "    # We test the healthy subjects\n",
    "    test_result_true_negative = 1-np.random.binomial(1, specificity, number_of_test_subjects - np.sum(ground_truth))\n",
    "\n",
    "    # We concatenate the results to get the test results of each subject\n",
    "    test_results = np.concatenate((test_result_true_negative, test_result_true_positive))\n",
    "\n",
    "    # From the ground truth and the test results, we can calculate p(subject is sick | test is positive)\n",
    "    prob_sick_if_positive = np.sum(ground_truth[test_results==1])/len(test_results[test_results==1])\n",
    "\n",
    "    return prob_sick_if_positive\n",
    "    \n",
    "def bayes(sensitivity, specificity, incidence_of_infection):\n",
    "    ## YOUR CODE GOES HERE\n",
    "    prior =  # p(sick)\n",
    "    evidence = # p(positive)\n",
    "    likelihood_fct = # p(test is positive | subject is sick)/p(positive)\n",
    "    posterior = # Bayes theorem\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = .975 # true positive rate\n",
    "specificity = .975 # true negative rate\n",
    "\n",
    "incidence_of_infection = 0.001 # #infections/#population\n",
    "number_of_test_subjects = 10000 # number of subjects in the test study\n",
    "\n",
    "## Run this to get the results! Try out different numbers for sensitivity, specificity, incidence, and number of test subjects!\n",
    "print('Simulation: p(subject is sick | test is positive) = {}'.format(simulate_test_development(sensitivity, specificity, incidence_of_infection, number_of_test_subjects)))\n",
    "print('Bayes theorem: p(subject is sick | test is positive) = {}'.format(bayes(sensitivity, specificity, incidence_of_infection)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63b590",
   "metadata": {},
   "source": [
    "### 2. Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845194f7",
   "metadata": {},
   "source": [
    "Lets have a look at random variables from different probability distributions. Here, we are looking at the binomial and the Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b71df",
   "metadata": {},
   "source": [
    "We start with the binomial distribution. The binomial distribution describes a sequence of random events, where each random event has two outcomes: one with probability $p$, and one with probability $1-p$.\n",
    "\n",
    "A simple example is tossing a coin, where $p = 0.5$. The binomial distribution $P(k\\ |\\ n, p) = \\binom{n}{k} p^k (1-p)^{n-k}$ gives us the probability that, when tossing $n$ coins, we find exactly $k$ times heads.\n",
    "\n",
    "**In the following, play around with the two parameters repeats ($n$) and prob ($p$). What happens for very large values of $n$? Does this remind you of another distribution you know?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# matplotlib and seaborn are used for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(424242)\n",
    "\n",
    "# Set up the parameters\n",
    "prob = 0.5\n",
    "repeats = 10\n",
    "num_samples = 100000\n",
    "\n",
    "# Sample 'num_samples' from a binomial distribution\n",
    "samples = np.random.binomial(repeats, prob, num_samples)\n",
    "\n",
    "# visualise the samples\n",
    "_ = plt.hist(samples, bins = 50, density=True, edgecolor = 'k', linewidth=0.5)\n",
    "plt.ylabel(r'$p(x)$', fontsize = 18, rotation = 0, labelpad=30)\n",
    "plt.xlabel(r'$x$', fontsize = 18)\n",
    "plt.title('Binomial distribution', fontsize = 18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e572b",
   "metadata": {},
   "source": [
    "Let's continue with another common probability distribution: the Poisson distribution. Assume we have certain events that can occur at any time, with an average rate of occurence $\\lambda > 0$. For example, this could be incoming phone calls in a call centre, radioactive decay of isotopes in a material, the number of spikes a cortical neuron receives from other neurons, etc. \n",
    "\n",
    "The Poisson distribution $p(k\\ |\\ \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$ yields the probability that in a given time interval, we observe $k$ such events. \n",
    "\n",
    "**What happens for large rates? Does the distribution seem familiar?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# matplotlib and seaborn are used for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(424242)\n",
    "\n",
    "# Set up the parameters\n",
    "lambdas = 1\n",
    "num_samples = 100000\n",
    "\n",
    "# Sample 'num_samples' from a Poisson distribution\n",
    "samples = np.random.poisson(lambdas, num_samples)\n",
    "\n",
    "# visualise the samples\n",
    "_ = plt.hist(samples, bins = 30, density=True, edgecolor = 'k', linewidth=0.5)\n",
    "plt.ylabel(r'$p(x)$', fontsize = 18, rotation = 0, labelpad=30)\n",
    "plt.xlabel(r'$x$', fontsize = 18)\n",
    "plt.title('Poisson distribution', fontsize = 18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9120ab2",
   "metadata": {},
   "source": [
    "**Curiosity:** During WW2, the Poisson distribution was used by British statisticians to identify that Nazi Germany did not possess advanced missile guidance systems, i.e., they dropped bombs randomly.\n",
    "See http://doi.org/10.1111/j.1740-9713.2019.01315.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b877311",
   "metadata": {},
   "source": [
    "### 3. Concentration bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75f16c",
   "metadata": {},
   "source": [
    "In this part, we will have a look at concentration bounds: Law of Large Numbers (LLN) and Chernoff Bounds.\n",
    "\n",
    "First, we explore this for a normal distribution with $\\mu = 0$ and $\\sigma^2 = 1$. We sample the mean for different numbers of $n$ (number of samples used to calculate the arithmetic mean), and compare our result with the above bounds.\n",
    "\n",
    "Moreover, we look at the distribution of the mean for $n=1$ and the maximum value of $n$ (here: $n = 2000$). As you will see, this distribution becomes narrower for increasing $n$!\n",
    "\n",
    "**Your task is to complete the function 'get_prob_that_mean_larger_epsilon', which calculates $p(|mean - E(x)| \\geq \\epsilon)$ from sampled data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8094540",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(424242)\n",
    "\n",
    "def LLN(epsilon, num_samples, var = 1):\n",
    "    return var/(epsilon**2*num_samples)\n",
    "\n",
    "def Chernoff(epsilon, num_samples, var=1):\n",
    "    return 2*np.exp(-epsilon**2*num_samples/(2*var))\n",
    "\n",
    "# Set up the parameters\n",
    "# Epsilon is the deviation from the mean\n",
    "epsilon = 0.1\n",
    "\n",
    "# Number of repeats for estimating probabilities\n",
    "repeats = 20000\n",
    "# Number of samples for each repeat. We are interested in the behaviour for different values.\n",
    "nvals = np.arange(1, 2001, 100)\n",
    "\n",
    "# This will be used to collect our experimental results for p(mean >= epsilon | num_samples)\n",
    "probabilities = []\n",
    "\n",
    "def get_prob_that_mean_larger_epsilon(samples, epsilon):\n",
    "    ## YOUR CODE GOES HERE\n",
    "    # Take the mean over the second dimension (the mean for each repeat)\n",
    "    mean = \n",
    "    # Check if the absolute value of the mean exceeds epsilon. This returns a list of booleans (True: exceeds, False: does not exceed)\n",
    "    check_if_larger_epsilon = \n",
    "    # To get the probability of exceeding the threshold, we take the mean of the boolean list (assuming True = 1, False = 0)\n",
    "    prob_to_be_larger = \n",
    "    return prob_to_be_larger, mean\n",
    "\n",
    "means = []\n",
    "for num_samples in nvals:\n",
    "    # sample (repeats, num_samples) values from a normal distribution with mean 0 and variance 1\n",
    "    # note: the first dimension (repeats) is used to estimate the probability of the mean exceeding epsilon, \n",
    "    # that is, we repeat the random experiment epsilon times. You could also use a loop here.\n",
    "    samples = np.random.normal(0,1, (repeats, num_samples))\n",
    "    prob_to_be_larger, sampled_means = get_prob_that_mean_larger_epsilon(samples, epsilon)\n",
    "    means.append(sampled_means)\n",
    "    probabilities.append(prob_to_be_larger)\n",
    "\n",
    "# visualisation of the probability that the arithmetic mean exceeds the expectation value by epsilon\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(nvals, LLN(epsilon, nvals), label = 'LLN', linewidth=4, linestyle='--', color = 'tomato')\n",
    "plt.plot(nvals, Chernoff(epsilon, nvals), label = 'Chernoff', linewidth=4, linestyle='dotted', color = 'steelblue')\n",
    "plt.plot(nvals, probabilities, label = 'Simulation', linewidth=4, color = 'k')\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.ylabel(r'$p\\left(\\left|\\frac{1}{n} \\sum_{i=1}^n x_i - \\text{E}(x_1)\\right| \\geq \\epsilon\\right)$', fontsize = 18, labelpad=10)\n",
    "plt.xlabel(r'$n$', fontsize = 18)\n",
    "plt.legend(frameon=False, fontsize=18)\n",
    "plt.yscale('log')\n",
    "sns.despine()\n",
    "\n",
    "# visualisation of the distribution of means\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "ax[0].hist(means[0], bins = np.arange(-5,5, 0.5), density=True, edgecolor = 'k', linewidth=0.5)\n",
    "ax[0].set_xlabel(r'arithmetic mean $\\bar{x}_n$ for $n=1$', fontsize = 18)\n",
    "ax[0].set_ylabel(r'$p(\\bar{x}_n)$', fontsize = 18)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "ax[1].hist(means[-1], bins = np.arange(-5,5, 0.05), density=True, edgecolor = 'k', linewidth=0.5)\n",
    "ax[1].set_xlabel(r'arithmetic mean $\\bar{x}_n$ for $n=2000$', fontsize = 18)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419f059",
   "metadata": {},
   "source": [
    "Lets apply our code to the Cauchy distribution. As you can see... the distribution of means does not change for increasing $n$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of repeats for estimating probabilities\n",
    "repeats = 20000\n",
    "# Number of samples for each repeat. We are interested in the behaviour for different values.\n",
    "nvals = np.arange(1, 2001, 100)\n",
    "\n",
    "means = []\n",
    "for num_samples in nvals:\n",
    "    # sample (repeats, num_samples) values from a normal distribution with mean 0 and variance 1\n",
    "    # note: the first dimension (repeats) is used to estimate the probability of the mean exceeding epsilon, \n",
    "    # that is, we repeat the random experiment epsilon times. You could also use a loop here.\n",
    "    samples = np.random.standard_cauchy((repeats, num_samples))\n",
    "    prob_to_be_larger, sampled_means = get_prob_that_mean_larger_epsilon(samples, epsilon)\n",
    "    means.append(sampled_means)\n",
    "\n",
    "# visualisation of the distribution of means\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "ax[0].hist(means[0], bins = np.arange(-10,10, 0.5), density=True, edgecolor = 'k', linewidth=0.5)\n",
    "ax[0].set_xlabel(r'arithmetic mean $\\bar{x}_n$ for $n=1$', fontsize = 18)\n",
    "ax[0].set_ylabel(r'$p(\\bar{x}_n)$', fontsize = 18)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "ax[1].hist(means[-1], bins = np.arange(-10,10, 0.5), density=True, edgecolor = 'k', linewidth=0.5)\n",
    "ax[1].set_xlabel(r'arithmetic mean $\\bar{x}_n$ for $n=2000$', fontsize = 18)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff9ede",
   "metadata": {},
   "source": [
    "### 4. Inverse transform sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a06681",
   "metadata": {},
   "source": [
    "In the lecture, we saw what happens to the probability distribution of a random variable $x$ if we apply an invertible function $f(x)$.\n",
    "\n",
    "In this exercise, we will turn this process around! Assume we have a random variable $x$ sampled from a uniform distribution. How do we have to choose $f$ such that $f(x)$ follows a desired probability distribution $p$?\n",
    "\n",
    "A technique solving this problem is 'inverse transform sampling':\n",
    "\n",
    "1) Calculate the cumulative distribution of $p$, denoted by $P(x') = \\int_{a}^{x'} p(x) dx$, where $a$ is the lower bound of the distribution. We choose $f = P^{-1}$.\n",
    "2) Generate a random number $x$ from the uniform distribution $U(0,1)$.\n",
    "3) Calculate $y = f(x)$. $y$ follows the probability distribution $p$.\n",
    "\n",
    "Why? Because we have\n",
    "- $x = f^{-1}(y) = P(y) \\leftrightarrow \\frac{dx}{dy} = p(y) \\leftrightarrow \\int 1 dx = \\int \\frac{dx}{dy} dy = \\int p(y) dy$\n",
    "\n",
    "**Meaning:** if x follows the uniform distribution (in $[0,1]$), then $y$ follows $p(y)$. The last step is exactly what we did in the lecture when transforming random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50185430",
   "metadata": {},
   "source": [
    "**We will try this for the exponential distribution** $p(y) = \\lambda e^{-\\lambda y}$ with $\\lambda > 0$ and $y \\geq 0$.\n",
    "\n",
    "What you have to do:\n",
    "\n",
    "Step 1: Calculate the cumulative distribution $P(x') = \\int_0^{x'} p(x) dx$\n",
    "\n",
    "Step 2: Invert $P$ to get $f = P^{-1}$. Complete 'function_f' using this expression.\n",
    "\n",
    "Step 3: Sample random numbers from a uniform distribution (number between 0 and 1) and apply $f$. This part is already implemented.\n",
    "\n",
    "Step 4: Compare the result with the exponential distribution (the red line in the figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195158fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_f(x, lambdas):\n",
    "    # YOUR CODE GOES HERE\n",
    "    # return the expression for f\n",
    "    return \n",
    "\n",
    "def exponential_distribution(x, lambdas):\n",
    "    return lambdas*np.exp(-lambdas*x)\n",
    "\n",
    "samples = np.random.uniform(size=(10000))\n",
    "transformed_samples = function_f(samples, 1)\n",
    "\n",
    "xvals = np.linspace(0, np.max(transformed_samples), 1000)\n",
    "plt.hist(function_f(samples, 1), bins = 50, density=True, color = 'steelblue', edgecolor = 'k', linewidth=0.5)\n",
    "plt.plot(xvals, exponential_distribution(xvals, 1), linewidth = 4, color = 'tomato')\n",
    "plt.xlabel(r'$y$', fontsize = 18)\n",
    "plt.ylabel(r'$p(y)$', fontsize = 18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ed451",
   "metadata": {},
   "source": [
    "### 5. Central limit theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5a015",
   "metadata": {},
   "source": [
    "The last example is the famous central limit theorem. We will look at it for the uniform distribution first.\n",
    "\n",
    "**Below is a setup for exploring the central limit theorem. Your task is to complete the function 'get_sum_of_uniforms'. The remaining code creates a histogram of the sum of random variables you obtained, and compares it with a Gaussian with zero mean and unit variance (red line)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(424242)\n",
    "\n",
    "num_of_rv_to_sum = 5\n",
    "\n",
    "def func(x, mu=0, sigma=1):\n",
    "    # Unit Gaussian function, used for visualisation.\n",
    "    return 1/np.sqrt(2*np.pi*sigma**2) *np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "def get_sum_of_uniforms(num_of_rv_to_sum, repeats = 10000):\n",
    "    ## YOUR CODE GOES HERE\n",
    "    # What we need is the following:\n",
    "    # repeat times, we sample 'num_of_rv_to_sum' numbers from a uniform distribution, \n",
    "    # sum them, and divide by the square root of num_of_rv_to_sum\n",
    "    # The output is a list of length 'repeats', i.e., we sample the sum 'repeats' times\n",
    "    \n",
    "    return \n",
    "\n",
    "# Get 'repeats' samples of the sum of random variables\n",
    "# We sum 'num_of_rv_to_sum' many random variables\n",
    "summed_rvs = get_sum_of_uniforms(num_of_rv_to_sum)\n",
    "# We center and rescale the final distribution, so that it has mean 0 and variance 1\n",
    "summed_rvs = summed_rvs - 0.5*np.sqrt(num_of_rv_to_sum)\n",
    "summed_rvs = summed_rvs/np.sqrt((1/12))\n",
    "\n",
    "# Visualise the distribution\n",
    "xval = np.linspace(-10, 10, 10000)\n",
    "_ = plt.hist(summed_rvs, bins = 100, density=True, color='steelblue', label = 'Histogram', edgecolor = 'k', linewidth=0.5)\n",
    "plt.plot(xval, func(xval), color = 'tomato', linewidth=2.5, label = 'Theory')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()\n",
    "plt.xlabel('x', fontsize = 20)\n",
    "plt.ylabel('p(x)', fontsize = 20, rotation = 0, labelpad=20)\n",
    "plt.xlim(-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3c555",
   "metadata": {},
   "source": [
    "The uniform distribution is already symmetric, but what happens for a distribution that is not symmetric? A good example is the exponential distribution!\n",
    "\n",
    "**Your task: implement the same function as before, but now sample the random variables from the exponential distribution. What happens for increasing values of 'num_of_rv_to_sum'? Can you explain this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(424242)\n",
    "\n",
    "num_of_rv_to_sum = 1\n",
    "lambdas = 2\n",
    "\n",
    "def func(x, mu=0, sigma=1):\n",
    "    # Unit Gaussian function\n",
    "    return 1/np.sqrt(2*np.pi*sigma**2) *np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "def get_sum_of_exponentials(num_of_rv_to_sum, repeats = 10000):\n",
    "    ## YOUR CODE GOES HERE\n",
    "    # Lets do the same, but for the exponential distribution\n",
    "    # It is given by np.random.exponential and has the 'scale' parameter,\n",
    "    # which we set to 1/lambdas (the mean of the distribution)\n",
    "    \n",
    "    return\n",
    "\n",
    "# Get 'repeats' samples of the sum of random variables\n",
    "# We sum 'num_of_rv_to_sum' many random variables\n",
    "summed_rvs = get_sum_of_exponentials(num_of_rv_to_sum)\n",
    "# We center and rescale the final distribution, so that it has mean 0 and variance 1\n",
    "summed_rvs = summed_rvs - 1/lambdas*np.sqrt(num_of_rv_to_sum)\n",
    "summed_rvs = summed_rvs/np.sqrt((1/lambdas**2))\n",
    "\n",
    "# Visualise the distribution\n",
    "xval = np.linspace(-10, 10, 10000)\n",
    "_ = plt.hist(summed_rvs, bins = 100, density=True, color='steelblue', label = 'Histogram', edgecolor = 'k', linewidth=0.5)\n",
    "plt.plot(xval, func(xval), color = 'tomato', linewidth=2.5, label = 'Theory')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()\n",
    "plt.xlabel('x', fontsize = 20)\n",
    "plt.ylabel('p(x)', fontsize = 20, rotation = 0, labelpad=20)\n",
    "plt.xlim(-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df0974",
   "metadata": {},
   "source": [
    "What happens if we use the Cauchy distribution instead...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67091563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(424242)\n",
    "\n",
    "num_of_rv_to_sum = 1\n",
    "\n",
    "def func(x, mu=0, sigma=1):\n",
    "    # Unit Gaussian function\n",
    "    return 1/np.sqrt(2*np.pi*sigma**2) *np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "def get_sum_of_cauchys(num_of_rv_to_sum, repeats = 10000):\n",
    "    ## YOUR CODE GOES HERE\n",
    "    # Lets do the same, but for the cauchy distribution\n",
    "\n",
    "    return \n",
    "\n",
    "# Get 'repeats' samples of the sum of random variables\n",
    "# We sum 'num_of_rv_to_sum' many random variables\n",
    "summed_rvs = get_sum_of_cauchys(num_of_rv_to_sum)\n",
    "# There is no defined mean or variance, so lets try centering using the observed mean and variance!\n",
    "summed_rvs = summed_rvs - np.mean(summed_rvs)\n",
    "summed_rvs = summed_rvs/np.std(summed_rvs)\n",
    "\n",
    "# Visualise the distribution\n",
    "xval = np.linspace(-10, 10, 10000)\n",
    "_ = plt.hist(summed_rvs, bins = 2000, density=True, color='steelblue', label = 'Histogram', edgecolor = 'k', linewidth=0.5)\n",
    "plt.plot(xval, func(xval), color = 'tomato', linewidth=2.5, label = 'Theory')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()\n",
    "plt.xlabel('x', fontsize = 20)\n",
    "plt.ylabel('p(x)', fontsize = 20, rotation = 0, labelpad=20)\n",
    "plt.xlim(-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88676fc0",
   "metadata": {},
   "source": [
    "### 6. Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f5775",
   "metadata": {},
   "source": [
    "To close this notebook, we take a brief look at actual data. In the following, we load data containing features extracted using recordings from the Hubble Space Telescope for sources (e.g., globular clusters, stars, background galaxies, etc.) in the galaxy 'NGC1427a'.\n",
    "\n",
    "Basically, we record the galaxy for two different frequency bands (i.e., we have two images of it, in the g and z band). After identifying the sources, we create small cut-outs. Thus, for every source we have two small images (the flux for each band)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dframe = pd.read_csv('NGC1427a_sources.csv')\n",
    "images = fits.getdata('NGC1427a_images.fits')\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,12))\n",
    "ax[0].imshow(images[115][0])\n",
    "ax[0].set_title('g-band image', fontsize = 20)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(images[115][1])\n",
    "ax[1].set_title('z-band image', fontsize = 20)\n",
    "ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c474a1",
   "metadata": {},
   "source": [
    "We then preprocess the data by extracting useful information from the images, such as\n",
    "\n",
    "- features that describe the shape (eccentricity, orientation, area, ...)\n",
    "- features that describe the flux (e.g., m3_g = brightness using a 3 pixel aperture in the g-band)\n",
    "\n",
    "Thus, we obtain a table containing several features for each source we identified. Just for fun, lets have a look at m3_g and m3_z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e483f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(dframe['m3_g'], dframe['m3_z'], alpha = 0.5, color = 'steelblue', edgecolors='k')\n",
    "plt.ylabel('m3_z', fontsize = 20, rotation = 0, labelpad=35)\n",
    "plt.xlabel('m3_g', fontsize = 20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93611336",
   "metadata": {},
   "source": [
    "It looks like the data follows some distribution. Lets try a multivariate Gaussian. We fit a 2D Gaussian using 'GaussianMixture' to the data, generate new samples from it, and compare the original data and our 'model-generated data' visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6), sharex=True, sharey=True)\n",
    "\n",
    "gmm = GaussianMixture(n_components=1).fit(dframe[['m3_g', 'm3_z']].values)\n",
    "model_samples, _ = gmm.sample(len(dframe))\n",
    "ax[0].scatter(dframe['m3_g'], dframe['m3_z'], alpha = 0.5, color = 'steelblue', edgecolors='k')\n",
    "ax[0].set_title('Original data', fontsize = 20)\n",
    "ax[1].scatter(model_samples[:,0], model_samples[:,1], color = 'tomato', alpha = 0.5, marker = 'o', edgecolor = 'k')\n",
    "ax[1].set_title('Data from single Gaussian', fontsize = 20)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce28397",
   "metadata": {},
   "source": [
    "Instead of assuming that the underlying data distribution is a single Gaussian, we can model the distribution using a weighted sum of Gaussians to be more flexible (an intuition: we can imagine that we have different types of sources (stars, galaxies, globular clusters, etc.) which follow different Gaussians!). You see the result below, which doesn't look too bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,6), sharex=True, sharey=True)\n",
    "\n",
    "gmm = GaussianMixture(n_components=6).fit(dframe[['m3_g', 'm3_z']].values)\n",
    "model_samples, _ = gmm.sample(len(dframe))\n",
    "ax[0].scatter(dframe['m3_g'], dframe['m3_z'], alpha = 0.5,  color = 'steelblue', edgecolors='k')\n",
    "ax[0].set_title('Original data', fontsize = 20)\n",
    "ax[1].scatter(model_samples[:,0], model_samples[:,1], color = 'tomato', alpha = 0.5, marker = 'o', edgecolor = 'k')\n",
    "ax[1].set_title('Data from mixture of Gaussians', fontsize = 20)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0709b735",
   "metadata": {},
   "source": [
    "Of course we only selected two out of many features here, but I hope this illustrates our assumption that data $x$ is generated from some underlying distribution $p(x)$. Our task is then to model this distribution using a parametrised function $f(x | \\theta)$ using parameters $\\theta$ (in case of a Gaussian mixture model, its the parameters of the Gaussians and the weighting factors of each Gaussian)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
