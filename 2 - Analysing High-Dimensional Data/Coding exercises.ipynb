{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b617ca2",
   "metadata": {},
   "source": [
    "## Code playground for the lecture block *Analysing High-Dimensional Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cdbb6",
   "metadata": {},
   "source": [
    "### 1. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2898b",
   "metadata": {},
   "source": [
    "In the following, you can play around with PCA. We will implement a simple version of PCA ourselves and use it to reduce the dimensionality of Pokémon images (from the original GameBoy game). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf661abf",
   "metadata": {},
   "source": [
    "First, we get the data. Make sure that the file 'Pokemon151.jpg' is in the same folder as this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1778ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image_path = './Pokemon151.jpg'\n",
    "image = np.array(Image.open(image_path).convert('L'))\n",
    "data = []\n",
    "\n",
    "# Cut the image to get pictures of individual Pokémon\n",
    "for i in range(10):\n",
    "    if i != 9:\n",
    "        jmax = 16\n",
    "    else:\n",
    "        jmax = 7\n",
    "    for j in range(jmax):\n",
    "        data.append(image[i*46:(i+1)*46,j*46:(j+1)*46]/255.)\n",
    "# We create a flattened version of the data (i.e., every image becomes a vector)\n",
    "# for later use in the algorithm.\n",
    "flattened_data = np.array(data).reshape(151, -1)\n",
    "\n",
    "# Look at the pictures (note: there is noise!)\n",
    "plt.imshow(data[150], cmap='grey')\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f8992",
   "metadata": {},
   "source": [
    "**Here, you have to implement the PCA method.**\n",
    "\n",
    "As a reminder, we require four steps:\n",
    "\n",
    "1) For each pixel, we calculate the mean over all data samples (here: 151 images of Pokémon). For convenience, we store these values in a vector of lengthh $46^2$.\n",
    "2) Subtract the mean from each data sample (element-wise). This way, we have $x_n - \\mu$, where $x_n$ is a vector storing the $n^\\text{th}$ image and $\\mu$ the mean vector.\n",
    "3) With this, we calculate the covariance matrix (averaged over all data samples). To get the covariance matrix, we have to form the outer product $(x_n - \\mu)(x_n - \\mu)^T$.\n",
    "4) Get the eigenvectors of the covariance matrix that correspond to the two largest eigenvalues. This is already implemented here using 'np.linalg.eigh'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad61e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE!\n",
    "\n",
    "# Calculate the mean value of each pixel (choose 'axis' correctly in np.mean!)\n",
    "# Use 'flattened_data' for this!\n",
    "mean_vector = \n",
    "# Subtract the mean from 'flattened_data'\n",
    "# Use 'centered_data' here, as we will use this vector later as well\n",
    "centered_data = \n",
    "\n",
    "# Calculate the covariance matrix (np.outer is useful for this!)\n",
    "covariance = \n",
    "\n",
    "# Spectral decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "\n",
    "# Sort the eigenvalues\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "# Get the two eigenvectors with the largest eigenvalues \n",
    "largest_eigvecs = eigenvectors[:, idx[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc86c5",
   "metadata": {},
   "source": [
    "First, we can have a look at the eigenvalues of the covariance matrix. They are sorted in descending order, with the first eigenvalue being, by far, the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.plot(eigenvalues[::-1], linewidth=0, color='steelblue', marker='o', markersize=7.5)\n",
    "plt.xlim(-1,30)\n",
    "plt.ylabel('Value', fontsize = 20)\n",
    "plt.xlabel('Eigenvalue index', fontsize = 20)\n",
    "sns.despine()\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d2cc3",
   "metadata": {},
   "source": [
    "We can also visualise the eigenvectors by reshaping them back into image shapes! Pokémon have quite diverse shapes, so it is hard to identify \"high-level features\" in the images. However, you can spot some shades resembling Pokémon in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4, figsize=(16,16))\n",
    "for j in range(4):\n",
    "    for i in range(4):\n",
    "        ax[j][i].imshow(np.reshape(eigenvectors[:, idx[j*4+i]], (46, 46)), cmap = 'gray')\n",
    "        ax[j][i].set_yticks([])\n",
    "        ax[j][i].set_xticks([])\n",
    "        ax[j][i].set_xlabel('Eigenvalue index: ' + str(idx[j*4+i]), fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fb21a",
   "metadata": {},
   "source": [
    "Finally, we use the two eigenvectors with largest eigenvalue to project each data sample into a two-dimensional space! This way, we can visualise the $46 \\times 46$ dimensional images in only two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import seaborn as sns\n",
    "\n",
    "projected_data = []\n",
    "for i in range(len(data)):\n",
    "    projected_data.append(centered_data[i] @ largest_eigvecs)\n",
    "projected_data = np.array(projected_data)\n",
    "\n",
    "def imscatter(x, y, images, ax=None, zoom=0.1):\n",
    "    \"\"\"\n",
    "    x, y     : coordinates\n",
    "    images   : list/array of image arrays (numpy or PIL)\n",
    "    ax       : matplotlib axis\n",
    "    zoom     : size of thumbnails\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    artists = []\n",
    "    for (x0, y0, img) in zip(x, y, images):\n",
    "        imagebox = OffsetImage(img, zoom=zoom, cmap='grey')\n",
    "        ab = AnnotationBbox(imagebox, (x0, y0), frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    return artists\n",
    "\n",
    "_ = imscatter(projected_data[:,0], projected_data[:,1], data, zoom=.9)\n",
    "plt.xlim((-10,12))\n",
    "plt.ylim((-12,10.5))\n",
    "plt.xlabel('First principal component', fontsize = 20)\n",
    "plt.ylabel('Second principal component', fontsize = 20, labelpad=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78039453",
   "metadata": {},
   "source": [
    "### 2. Marcenko-Pastur distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0241a9",
   "metadata": {},
   "source": [
    "We will have a brief look at the Marcenko-Pastur distribution. As in the lecture, we generate data points from a Gaussian distribution, and we \"spike\" the correlation matrix into some direction. If this perturbation is strong enough, it will manifest as an eigenvalue outside of the Marcenko-Pastur distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(42)\n",
    "\n",
    "def marcenko_pastur(x, gamma):\n",
    "    # YOUR CODE GOES HERE!\n",
    "    # Add the formula of the Marcenko-Pastur distribution\n",
    "    gammap = \n",
    "    gammam = \n",
    "\n",
    "    return \n",
    "\n",
    "# Strength of the component we add\n",
    "beta = 1.5\n",
    "# Variance of random data\n",
    "sigma = 1\n",
    "# Dimension of data points\n",
    "dim = 2000\n",
    "# Number of data points\n",
    "num_samples = 3000\n",
    "# Gamma as defined in the lecture\n",
    "gamma = dim/num_samples\n",
    "    \n",
    "# The principal component we are adding\n",
    "component = np.ones(dim)\n",
    "component = component/np.linalg.norm(component)\n",
    "\n",
    "# Generation of random samples with principal component\n",
    "mean = np.zeros(dim)\n",
    "cov = np.eye(dim) + beta *np.outer(component, component)\n",
    "points = np.random.multivariate_normal(mean, cov, num_samples)\n",
    "\n",
    "# YOUR CODE GOES HERE!\n",
    "# Calculate the covariance matrix from the data points and\n",
    "# get its eigenvalues and eigenvectors (just like for PCA)\n",
    "covariance = \n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(eigenvalues, bins = 200, density=True)\n",
    "\n",
    "vals = np.linspace(0,12,10000)\n",
    "plt.plot(vals, marcenko_pastur(vals, gamma), linewidth=2.5, color = 'tomato')\n",
    "\n",
    "plt.ylabel(r'$p(\\lambda)$', fontsize=20)\n",
    "plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "plt.xlim(-0.005,5)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05959356",
   "metadata": {},
   "source": [
    "### 3. Random projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0527490",
   "metadata": {},
   "source": [
    "Time to look at the power of random projections! As we saw in the lecture, random projections can have strong guarantees for conserving vector lengths and vector distances. This is especially useful if we are faced with very high-dimensional data, and we want to find or group data points that are similar to each other. We can perform these operations in the projected subspace!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9865e",
   "metadata": {},
   "source": [
    "We will evaluate the Johnson-Lindenstrauss lemma here numerically. First, we load the famous MNIST handwritten digits dataset using the pytorch library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# load mnist data from pytorch\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# data  loader\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491915e",
   "metadata": {},
   "source": [
    "Lets visualise some data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f93c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,20, figsize=(20,5))\n",
    "\n",
    "for i in range(20):\n",
    "    ax[i].imshow(mnist_trainset[i][0][0], cmap='gray_r')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027b740",
   "metadata": {},
   "source": [
    "Time to project the data! You can change the parameters to adjust the quality of the projections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ac134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "m = 200\n",
    "# Error\n",
    "epsilon = 0.5\n",
    "# Dimension of the subspace\n",
    "d = int(4*np.log(m)/epsilon**2)\n",
    "# Probability that distances and lengths are preserved\n",
    "prob = 1-2/m \n",
    "print('At dimension {}, lengths and distances of data points are preserved with probability at least {}.'.format(d, prob))\n",
    "\n",
    "# Create random projections\n",
    "np.random.seed(4242)\n",
    "# YOUR CODE GOES HERE!\n",
    "# Create the projection matrix as in the lecture (random numbers from a Gaussian with zero mean and variane 1, rescaled by 1/sqrt(d))\n",
    "projection_operator = \n",
    "\n",
    "# data loader, we will only use the first batch of size \"m\"\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=m, shuffle=True)\n",
    "\n",
    "# For every data point in the dataset (trainloader), project it using M.\n",
    "for data, _ in trainloader:\n",
    "    # Some pytorch magic... we switch to numpy here for simplicity. Torch is only used to get the data.\n",
    "    data = data.squeeze(1)\n",
    "    data = data.view(-1, 28*28)\n",
    "    data = data.numpy()\n",
    "    # YOUR CODE GOES HERE!\n",
    "    # Project the data using the projection_operator\n",
    "    transformed = \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699068d",
   "metadata": {},
   "source": [
    "To evaluate the results, we need the differences between the original data points, as well as the differences between the projected data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_theory = []\n",
    "diffs_actual = []\n",
    "for i in range(m):\n",
    "    for j in range(i+1, m):\n",
    "        # YOUR CODE GOES HERE!\n",
    "        # Calculate the distance between the projected data points\n",
    "        diffs_theory.append(...)\n",
    "        # Calculate the distance between the original data points\n",
    "        diffs_actual.append(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e94a5",
   "metadata": {},
   "source": [
    "Lets plot the results! The dashed lines are the bounds of the Johnson-Lindenstrauss lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We look at the ratio of the length of the original data points and their projected versions\n",
    "plt.plot((np.linalg.norm(transformed, axis=1)/np.linalg.norm(data, axis=1)), color='steelblue', linewidth = 2.5)\n",
    "plt.hlines(1+epsilon, 0, m-1, color = 'k', linewidth=2.5, linestyles='--')\n",
    "plt.hlines(1-epsilon, 0, m-1, color = 'k', linewidth=2.5, linestyles='--')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.xlabel('Training samples', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# Same for the distances between data points\n",
    "plt.plot(np.array(diffs_theory)/np.array(diffs_actual), color='steelblue', linewidth = 2.5)\n",
    "plt.hlines(1+epsilon, 0, len(diffs_actual), color = 'k', linewidth=2.5, linestyles='--')\n",
    "plt.hlines(1-epsilon, 0, len(diffs_actual), color = 'k', linewidth=2.5, linestyles='--')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.xlabel('Pairwise combinations', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a757e8",
   "metadata": {},
   "source": [
    "### 4. k-means clustering\n",
    "\n",
    "Time to do some clustering. First, we will implement one of the most classic clustering algorithms ourselves. For convex clusters, this algorithm works well. However, as we will see, for non-convex ones, it struggles. To improve the method, we will upgrade it in the section that follows using spectral clustering! **In this part, your task is to complete the k-means function!**\n",
    "\n",
    "First, we get some data to play with. We get these from the Python library sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c727318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "n_samples = 500\n",
    "seed = 30\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n",
    ")\n",
    "\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=442\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a192ace",
   "metadata": {},
   "source": [
    "Lets have a look at this data! For simplicity, it is only two-dimensional data. By eye, we can immediately identify \"clusters\" of data that belong together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7933b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(noisy_circles[0][:,0], noisy_circles[0][:,1], edgecolors='k', s=200)\n",
    "plt.title('Noisy circles', fontsize= 14)\n",
    "sns.despine()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(varied[0][:,0], varied[0][:,1], edgecolors='k', s=200)\n",
    "plt.title('Varied convex clusters', fontsize= 14)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c345e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(dataset, num_clusters, iters = 10, dim=2, seed = 4242):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ## INITIALISATION\n",
    "\n",
    "    # clusters_mean contains the mean vector of each cluster\n",
    "    # we set it to 0 for now and then initialise it\n",
    "    clusters_mean = np.zeros((num_clusters, dim))\n",
    "    # YOUR CODE GOES HERE\n",
    "    # Assign random clusters to all points\n",
    "    # For instance, if we try to find 3 clusters, we assign each\n",
    "    # data point a label in [0,1,2]\n",
    "    random_init = ...\n",
    "    # Calculate the mean vector for each cluster\n",
    "    # I.e., cluster_mean[0] = mean of all data points assigned label 0 etc. \n",
    "    ...\n",
    "\n",
    "    # K-MEANS\n",
    "\n",
    "    # Lets start with the algorithm! We loop 'iter' amount of times (to avoid endless loops...)\n",
    "    for _ in range(iters):\n",
    "        # labels contains the cluster assignment\n",
    "        labels = []\n",
    "\n",
    "        # loop over all data points\n",
    "        for data_sample in dataset:\n",
    "            # YOUR CODE GOES HERE!\n",
    "            # For data_sample, calculate its distance to all cluster centres (clusters_mean)\n",
    "            distances = ...\n",
    "            # Assign the label corresponding to the cluster centre that is closest\n",
    "            # I.e., we are looking for the argmin of distances!\n",
    "            labels.append(...)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Recalculate the mean vector for each cluster\n",
    "        # HERE GOES the same code you wrote for the initialisation, but\n",
    "        # we use 'labels' now!\n",
    "        for i in range(num_clusters):\n",
    "            if len(dataset[np.array(labels)==i]) != 0:\n",
    "                clusters_mean[i] = np.mean(dataset[np.array(labels)==i], axis=0)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c2677",
   "metadata": {},
   "source": [
    "Lets try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a25be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = varied[0]\n",
    "num_clusters = 3\n",
    "\n",
    "clusters = kmeans(dataset, num_clusters)\n",
    "\n",
    "colors = ['orange', 'tomato', 'steelblue']\n",
    "for i in range(num_clusters):\n",
    "    data_cluster = dataset[np.array(clusters)==i]\n",
    "    plt.scatter(data_cluster[:,0], data_cluster[:,1], color=colors[i], edgecolors='k', s=200)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ebb2b",
   "metadata": {},
   "source": [
    "You should see three nicely identified clusters! :) Lets try the other dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = noisy_circles[0]\n",
    "num_clusters = 2\n",
    "\n",
    "clusters = kmeans(dataset, num_clusters)\n",
    "\n",
    "colors = ['orange', 'tomato', 'steelblue']\n",
    "for i in range(num_clusters):\n",
    "    data_cluster = dataset[np.array(clusters)==i]\n",
    "    plt.scatter(data_cluster[:,0], data_cluster[:,1], color=colors[i], edgecolors='k', s=200)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70955a4a",
   "metadata": {},
   "source": [
    "That... did not work :( But we can fix this with a simple adjustment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdce0e0",
   "metadata": {},
   "source": [
    "### 5. Spectral clustering\n",
    "\n",
    "In spectral clustering, we first find a new representation of our data coming from the spectral decomposition of the graph Laplacian. **Your task is to upgrade the k-means method you implemented to spectral clustering!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, calculate the similarity between data points\n",
    "# Gamma is a hyperparameter that controls how far points can be apart from each other and still be considered similar\n",
    "gamma = 0.1 # For varied, gamma = 1 works well. For circles, gamma = 0.1\n",
    "dataset = noisy_circles[0]\n",
    "num_clusters = 2\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "# Calculate the similarity of data points\n",
    "# We use a Gaussian kernel for this: similarity(x,y) = exp(-(x-y))^2/(2*gamma^2)\n",
    "distances = ...\n",
    "# Make sure that the diagonal of distances is 0!\n",
    "...\n",
    "# Now lets calculate the degree matrix\n",
    "degree = ...\n",
    "# The Laplacian is then obtained by taking the difference\n",
    "laplacian = degree - distances\n",
    "\n",
    "# All that remains to be done is the spectral decomposition. \n",
    "eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "sorted_ids = np.argsort(eigenvalues)\n",
    "# Different from PCA, we take the eigenvectors with the smallest eigenvalues!\n",
    "transformed_data = eigenvectors[:, :num_clusters]\n",
    "\n",
    "# And now, we simply do k-means using our transformed data!\n",
    "clusters = kmeans(transformed_data, num_clusters, dim=num_clusters)\n",
    "\n",
    "colors = ['orange', 'tomato', 'steelblue']\n",
    "for i in range(num_clusters):\n",
    "    data_cluster = dataset[np.array(clusters)==i]\n",
    "    plt.scatter(data_cluster[:,0], data_cluster[:,1], color=colors[i], edgecolors='k', s=200)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0d322",
   "metadata": {},
   "source": [
    "### 6. Diffusion maps / Laplacian Eigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f167f",
   "metadata": {},
   "source": [
    "As a last exercise, let's implement diffusion maps! **Your task is to fill out the missing code for constructing the Laplacian!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21204043",
   "metadata": {},
   "source": [
    "First, load some interesting data. We will use the S-curve in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset, colors = make_s_curve(n_samples=2000, random_state = 42)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(\n",
    "    dataset[:, 0], dataset[:, 1], dataset[:, 2], c=colors, s=50, alpha=0.8\n",
    ")\n",
    "ax.view_init(azim=-66, elev=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d196231",
   "metadata": {},
   "source": [
    "Second, we implement the algorithm and plot our results :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "# First, calculate the similarity between data points\n",
    "# Gamma is a hyperparameter that controls how far points can be apart from each other and still be considered similar\n",
    "gamma = .2 # For varied, gamma = 1 works well. For circles, gamma = 0.1\n",
    "reduced_dim = 2\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "# Calculate the similarity of data points\n",
    "# We use a Gaussian kernel for this: similarity(x,y) = exp(-(x-y))^2/(2*gamma^2)\n",
    "# (This is the same as for Spectral Clustering)\n",
    "distances = \n",
    "# Make sure that the diagonal of distances is 0!\n",
    "\n",
    "# Now lets calculate the degree matrix\n",
    "degree = \n",
    "# And its inverse\n",
    "degree_inv = \n",
    "\n",
    "# The Laplacian is obtained by the matrix product of degree_inv and distances\n",
    "M = \n",
    "# The symmetrised Laplacian is given by\n",
    "S = np.sqrt(degree) @ M @ np.sqrt(degree_inv)\n",
    "\n",
    "# All that remains to be done is the spectral decomposition. \n",
    "eigenvalues, eigenvectors = np.linalg.eigh(S)\n",
    "eigenvectors = np.sqrt(degree_inv) @ eigenvectors\n",
    "# # Different from PCA, we take the eigenvectors with the smallest eigenvalues!\n",
    "transformed_data = eigenvectors[:, -3:-1]\n",
    "transformed_data[:, 0] = eigenvalues[-3] * transformed_data[:,0]\n",
    "transformed_data[:, 1] = eigenvalues[-2]* transformed_data[:,1]\n",
    "\n",
    "# Time to plot our result!\n",
    "_ = plt.scatter(transformed_data[:,0], transformed_data[:,1], c=colors, s=50, alpha=0.8)\n",
    "_ = plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
